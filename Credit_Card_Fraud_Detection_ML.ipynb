{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Credit Card Fraud Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1tDUx5fTvWpKWIpqfwT_lTlfsBqIBe0G0",
      "authorship_tag": "ABX9TyPtZNqIX9LNEbog6vOuU161",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicardoLeonCo/MachineLearningPortfolio/blob/main/Credit_Card_Fraud_Detection_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXBU-HzC9ddd"
      },
      "source": [
        "#STEP 1: IMPORTING PACKAGES\n",
        "\n",
        "import pandas as pd # this package is for data processing\n",
        "import numpy as np #library to work with arrays\n",
        "import matplotlib.pyplot as plt #library to create visualizations in Python\n",
        "from termcolor import colored as cl #tool for text customization\n",
        "import itertools #library in Python consisting of multiple methods used in iterators\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler #Tool for data normalization/standardization\n",
        "from sklearn.model_selection import train_test_split #function that splits data arrays into 2 subsets: training & testing\n",
        "from sklearn.tree import DecisionTreeClassifier #Decision tree algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier #algorithm that implements learning based on the k nearest neighbors\n",
        "from sklearn.linear_model import LogisticRegression #Logistic regression algorithm (ML algorithm used to predict the probability of a categorical dependent variable)\n",
        "from sklearn.svm import SVC #Stands for \"Support Vector Classification\", and it's an SVM algorithm\n",
        "from sklearn.ensemble import RandomForestClassifier #Random forest tree algorithm\n",
        "from xgboost import XGBClassifier #Machine Learning Algorithm.\n",
        "\n",
        "from sklearn.metrics import confusion_matrix #evaluation metric\n",
        "from sklearn.metrics import accuracy_score #evaluation metric\n",
        "from sklearn.metrics import f1_score #evaluation metric"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17925EC49_Ez"
      },
      "source": [
        "**Notes:**\n",
        "- Iterators are objects that allow us to iterate over all the elements of a collection and return one element at a time.\n",
        "- When a dataset contains variable that are in different scales, *StandardScaler* performs the task of **Standardization** so they have a common scale.\n",
        "- **Support Vector Machines** are supervised learning methods used for classification.\n",
        "- **Random Forest Classifier** creates a set of decision trees from a random subset of the training set.\n",
        "- **XGBoost** is an implementation of gradient boosted decision trees designed for speed and performance.\n",
        "- A **Confusion Matrix** is a summary of prediction results on a classification problem.\n",
        "- Accuracy is simply a ratio of correctly predicted observations to the total observations.\n",
        "- Also known as F-score, the **F1 Score** is a weighted average of the precision and recall score. Used as an evaluation metric, a high F-score is a sign of a well-performing model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ougfKzMR-EHZ",
        "outputId": "3e482698-33d0-4905-f4d0-616648cf3da7"
      },
      "source": [
        "#Now I'm going to import my data from the Kaggle dataset.\n",
        "\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "df.drop('Time', axis = 1, inplace = True)\n",
        "df.head"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of               V1        V2        V3  ...       V28  Amount  Class\n",
              "0      -1.359807 -0.072781  2.536347  ... -0.021053  149.62    0.0\n",
              "1       1.191857  0.266151  0.166480  ...  0.014724    2.69    0.0\n",
              "2      -1.358354 -1.340163  1.773209  ... -0.059752  378.66    0.0\n",
              "3      -0.966272 -0.185226  1.792993  ...  0.061458  123.50    0.0\n",
              "4      -1.158233  0.877737  1.548718  ...  0.215153   69.99    0.0\n",
              "...          ...       ...       ...  ...       ...     ...    ...\n",
              "213964 -0.313355  1.038943 -0.093972  ...  0.032668    9.12    0.0\n",
              "213965  2.030213  0.285221 -1.938915  ... -0.068185    2.90    0.0\n",
              "213966  0.142229  0.672251  0.983900  ...  0.289856    8.57    0.0\n",
              "213967  2.144033 -1.953243 -0.578686  ... -0.033122   84.00    0.0\n",
              "213968  2.157143 -0.687990 -2.032580  ...       NaN     NaN    NaN\n",
              "\n",
              "[213969 rows x 30 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jQRiwaRXKqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea667dad-519c-410d-8d9f-be14551badd2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHfDAc-X7oiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ddc7f93-d0d0-4b7e-d166-16d5d806ef9f"
      },
      "source": [
        "cases = len(df)\n",
        "nonfraud_count = len(df[df.Class == 0])\n",
        "fraud_count = len(df[df.Class == 1])\n",
        "fraud_percentage = round(fraud_count/nonfraud_count*100, 2)\n",
        "\n",
        "print(cl('CASE COUNT', attrs = ['bold']))\n",
        "print(cl('--------------------------------------------------', attrs= ['bold']))\n",
        "print(cl('Total number of cases are {}'.format(cases), attrs = ['bold']))\n",
        "print(cl('Number of Non-fraud cases are {}'.format(nonfraud_count), attrs = ['bold']))\n",
        "print(cl('Percentage of fraud cases is {}'.format(fraud_percentage), attrs = ['bold']))\n",
        "print(cl('--------------------------------------------------', attrs = ['bold']))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mCASE COUNT\u001b[0m\n",
            "\u001b[1m--------------------------------------------------\u001b[0m\n",
            "\u001b[1mTotal number of cases are 213969\u001b[0m\n",
            "\u001b[1mNumber of Non-fraud cases are 213570\u001b[0m\n",
            "\u001b[1mPercentage of fraud cases is 0.19\u001b[0m\n",
            "\u001b[1m--------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kH31BA4n31g"
      },
      "source": [
        "**Notes:**\n",
        "- When working with the **print statement** I am wondering... *What does the \"cl\" stand for?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3G8VpVzptYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396a6b85-3417-4619-f612-82105a2e3a8a"
      },
      "source": [
        "nonfraud_cases = df[df.Class == 0]\n",
        "fraud_cases = df[df.Class == 1]\n",
        "\n",
        "#What are we doing here? Are we creating a class or some sort\n",
        "#of boolean?\n",
        "\n",
        "print(cl('CASE AMOUNT STATISTICS', attrs = ['bold']))\n",
        "print(cl('--------------------------------------------------', attrs= ['bold']))\n",
        "print(cl('NON-FRAUD CASE AMOUNT STATS', attrs= ['bold']))\n",
        "print(nonfraud_cases.Amount.describe())\n",
        "print(cl('--------------------------------------------------', attrs= ['bold']))\n",
        "print(cl('FRAUD CASE AMOUNT STATS', attrs= ['bold']))\n",
        "print(fraud_cases.Amount.describe())\n",
        "print(cl('--------------------------------------------------', attrs= ['bold']))\n",
        "\n",
        "#I think what this is doing is to call the describe function \n",
        "#with amounts. But what does *.amount* do?"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mCASE AMOUNT STATISTICS\u001b[0m\n",
            "\u001b[1m--------------------------------------------------\u001b[0m\n",
            "\u001b[1mNON-FRAUD CASE AMOUNT STATS\u001b[0m\n",
            "count    213570.000000\n",
            "mean         90.156783\n",
            "std         248.673400\n",
            "min           0.000000\n",
            "25%           6.000000\n",
            "50%          23.165000\n",
            "75%          79.640000\n",
            "max       19656.530000\n",
            "Name: Amount, dtype: float64\n",
            "\u001b[1m--------------------------------------------------\u001b[0m\n",
            "\u001b[1mFRAUD CASE AMOUNT STATS\u001b[0m\n",
            "count     398.000000\n",
            "mean      123.841307\n",
            "std       257.528158\n",
            "min         0.000000\n",
            "25%         1.000000\n",
            "50%        13.385000\n",
            "75%       105.890000\n",
            "max      2125.870000\n",
            "Name: Amount, dtype: float64\n",
            "\u001b[1m--------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUT5RwQ6tiqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5587669-3063-4400-a84c-5f28f582aad8"
      },
      "source": [
        "#Now we are going to normalize our data because we need to\n",
        "#reduce the wide range of values in the \"Amount\" variable to\n",
        "#a smaller scale so we can work better.\n",
        "\n",
        "sc = StandardScaler()\n",
        "amount = df['Amount'].values\n",
        "\n",
        "df['Amount'] = sc.fit_transform(amount.reshape(-1, 1))\n",
        "\n",
        "print(cl(df['Amount'].head(10), attrs = ['bold']))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m0    0.238851\n",
            "1   -0.351958\n",
            "2    1.159825\n",
            "3    0.133822\n",
            "4   -0.081343\n",
            "5   -0.348017\n",
            "6   -0.342709\n",
            "7   -0.198716\n",
            "8    0.011985\n",
            "9   -0.347977\n",
            "Name: Amount, dtype: float64\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRmzsIfhCzOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12fa5b1-b349-4f25-83eb-38d686b32f3a"
      },
      "source": [
        "# DATA SPLIT\n",
        "#Defining the independent and dependent variables.\n",
        "\n",
        "X = df.drop('Class', axis = 1).values\n",
        "y = df['Class'].values\n",
        "\n",
        "#What does this line of code below mean?\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "print(cl('X_train samples : ', attrs= ['bold']), X_train[:1])\n",
        "print(cl('X_test samples : ', attrs= ['bold']), X_test[0:1])\n",
        "print(cl('y_train samples : ', attrs=['bold']), y_train[0:20])\n",
        "print(cl('y_test samples: ', attrs = ['bold']), y_test[0:20])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mX_train samples : \u001b[0m [[-3.64971301  1.03705813 -2.61098587 -0.24862292  0.40450725 -2.17045741\n",
            "   0.51355845  1.05182087 -0.9940041  -0.80980017 -1.34569008  0.91219019\n",
            "   0.61394201  1.79762039 -0.51427845 -0.5812453   0.48558451 -0.44031489\n",
            "  -0.1857532  -0.28375743  0.38628129  0.93250574 -1.09799487  0.12276579\n",
            "   1.07470962  0.12621333  0.09688667 -0.35373672 -0.20193331]]\n",
            "\u001b[1mX_test samples : \u001b[0m [[-0.55972084  1.28998642 -0.10024787 -0.70158847  0.10509065 -0.91513154\n",
            "   0.56998271  0.34510566 -0.36110484 -0.17027319  0.81275162  1.28472114\n",
            "   0.4543221   0.46342535 -1.18854628  0.15344017 -0.49328117 -0.32174108\n",
            "   0.13251209  0.01514434 -0.1857103  -0.43463981  0.1250484   0.07906838\n",
            "  -0.44060151  0.12049292  0.22733596  0.08681519 -0.35481244]]\n",
            "\u001b[1my_train samples : \u001b[0m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\u001b[1my_test samples: \u001b[0m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzNlL5s7E3am"
      },
      "source": [
        "We have all the required components to build our classification models, which is our next step.\n",
        "\n",
        "# **Building the Model**\n",
        "\n",
        "For this project we will build 6 different types of classification models using algorithms by scikit-learn.\n",
        "\n",
        "1. Decision Tree\n",
        "2. K-Nearest Neighbors (KNN)\n",
        "3. Logistic Regression\n",
        "4. Support Vector Machine (SVM)\n",
        "5. Random Forest\n",
        "6. XGBoost\n",
        "\n",
        "# **Evaluation Metrics for Classification Models**\n",
        "\n",
        "We will use the following metrics to evaluate the models and decide which one is the best#\n",
        "\n",
        "+ Accuracy Score\n",
        "+ F1 Score\n",
        "+ Confusion Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUNPbgFctThU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "f839ac13-586b-4696-dfc6-8e1cd33b7292"
      },
      "source": [
        "# Models\n",
        "\n",
        "# 1. Decision Tree\n",
        "\n",
        "#When max_depth = 4 we allow the tree to split four times\n",
        "#Criterion is a parameter that measures the quality of a split\n",
        "#in our decision trees. \"Gini Index\" & \"Entropy\" are two\n",
        "#different measures of impurity or disorder*?\n",
        "tree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\n",
        "tree_model.fit(X_train, y_train)\n",
        "tree_yhat = tree_model.predict(X_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c8c1090c2f30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#different measures of impurity or disorder*?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtree_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'entropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtree_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtree_yhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m         )\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             X, y = self._validate_data(\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             )\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0;31m# :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    114\u001b[0m             raise ValueError(\n\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 )\n\u001b[1;32m    118\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i55Ts6EeyTdi"
      },
      "source": [
        "#Let's print the evaluation metrics for this model.\n",
        "\n",
        "# 1. Accuracy score\n",
        "\n",
        "print(cl('ACCURACY SCORE', attrs = ['bold']))\n",
        "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
        "print(cl('Accuracy score of the Decision Tree Model is {}'.format(accuracy_score(y_test, tree_yhat)), attrs= ['bold']))\n",
        "print(cl('------------------------------------------------------------------------', attrs = ['bold']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwhmZ7cGvHLP"
      },
      "source": [
        "#Models\n",
        "\n",
        "#2. K-Nearest Neighbors\n",
        "\n",
        "n = 5\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors = n)\n",
        "knn.fit(X_train, y_train)\n",
        "knn_yhat = knn.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeZ-gYiSmceo"
      },
      "source": [
        "#Evaluation Metrics from this metric\n",
        "\n",
        "print(cl('Accuracy score of the KNN model is {}'.format(accuracy_score(y_test, knn_yhat)), attrs = ['bold'], color = 'green'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjYEeLJvijxU"
      },
      "source": [
        "#Models\n",
        "\n",
        "#3. Logistic Regression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr_yhat = lr.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FScfqVOri3P"
      },
      "source": [
        "print(cl('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_yhat)), attrs= ['bold']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWbT0coeYOV2"
      },
      "source": [
        "#Models\n",
        "\n",
        "#4. SVM\n",
        "\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "svm_yhat = svm.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj3Dn3ffY03c"
      },
      "source": [
        "print(cl('Accuracy score of the SVM model is{}'.format(accuracy_score(y_test, svm_yhat)), attrs = ['bold']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZEfbyXJsjXj"
      },
      "source": [
        "#Models\n",
        "\n",
        "#5. Random Forest Tree\n",
        "\n",
        "rf = RandomForestClassifier(max_depth = 4)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_yhat = rf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT0eK3wFtP9h"
      },
      "source": [
        "print(cl('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test, rf_yhat)), attrs = ['bold']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se-Hssk4yZSG"
      },
      "source": [
        " #Models \n",
        "\n",
        " #6. XGBoost\n",
        "\n",
        " xgb = XGBClassifier(max_depth = 4)\n",
        " xgb.fit(X_train, y_train)\n",
        " xgb_yhat = xgb.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqQ9lbdt1o3g"
      },
      "source": [
        "print(cl('Accuracy score of the XGBoost model is {}'.format(accuracy_score(y_test, xgb_yhat)), attrs = ['bold']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id9euagE1_kM"
      },
      "source": [
        "The model with the least accuracy score is the Logistic Regression model whereas the model with the highest accuracy score is the KNN.\n",
        "\n",
        "Another accuracy metric widely used in evaluating models is the f1 score. You calculate it by dividing the product F1 score = 2((precision * recall) / (precision + recall))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMV7Z3gx2T71"
      },
      "source": [
        "# 2. F1 scores\n",
        "\n",
        "print(cl('F1 SCORES', attrs = ['bold']))\n",
        "\n",
        "print(cl('F1 score of the Decision Tree model is {}'.format(f1_score(y_test, tree_yhat)), attrs = ['bold']))\n",
        "print(cl('F1 score of the KNN model is {}'.format(f1_score(y_test, knn_yhat)), attrs = ['bold'], color = 'green'))\n",
        "print(cl('F1 score of the Logistic Regression model is {}'.format(f1_score(y_test, lr_yhat)), attrs = ['bold'], color = 'red'))\n",
        "print(cl('F1 score of the SVM model is {}'.format(f1_score(y_test, svm_yhat)), attrs = ['bold']))\n",
        "print(cl('F1 score of the Random Forest Tree model is {}'.format(f1_score(y_test, rf_yhat)), attrs = ['bold']))\n",
        "print(cl('F1 score of the XGBoost model is {}'.format(f1_score(y_test, xgb_yhat)), attrs = ['bold']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNKRyoLDlh_e"
      },
      "source": [
        "# **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-VBT5p-lcDb"
      },
      "source": [
        "#3. Confusion Matrix\n",
        "\n",
        "#defining the plot function\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, title, normalize = False, cmap = plt.cm.Blues):\n",
        "  title = 'Confusion Matrix of {}'.format(title)\n",
        "  if normalize:\n",
        "    cm = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
        "    #cmap stands for color map and it's part of a library. Matplotlib?\n",
        "    plt.title(title) #plt is the pyplot package that we imported at the beginning of our code.\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation = 45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "      plt.text(j, i, format(cm[i, j], fmt),\n",
        "               horizontalalignment = 'center',\n",
        "               color = 'white' if cm[i, j] > thresh else 'black')\n",
        "      \n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "#Now that we have the code above we are going to compute the confusion matrix for the models\n",
        "\n",
        "tree_matrix = confusion_matrix(y_test, tree_yhat, labels = [0, 1]) # Decision Tree\n",
        "knn_matrix = confusion_matrix(y_test, knn_yhat, labels = [0, 1]) #K-Nearest Neighbors\n",
        "lr_matrix = confusion_matrix(y_test, lr_yhat, labels = [0, 1]) #Logistic Regression\n",
        "svm_matrix = confusion_matrix(y_test, svm_yhat, labels = [0, 1]) #Support Vector Machine\n",
        "rf_matrix = confusion_matrix(y_test, rf_yhat, labels = [0, 1]) #Random Forest Tree\n",
        "xgb_matrix = confusion_matrix(y_test, xgb_yhat, labels = [0, 1]) #XGBoost\n",
        "\n",
        "#Plot each confusion matrix\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (6,6)\n",
        "\n",
        "#1. Decision tree\n",
        "\n",
        "tree_cm_plot = plot_confusion_matrix(tree_matrix,\n",
        "                                     classes = ['Non-Default(0)', 'Default(1)'],\n",
        "                                     normalize = False, title = 'Decision Tree')\n",
        "plt.savefig('tree_cm_plot.png')\n",
        "plt.show()\n",
        "\n",
        "#2. K-Nearest Neighbors\n",
        "\n",
        "knn_cm_plot = plot_confusion_matrix(knn_matrix,\n",
        "                                    classes = ['Non-default(0)', 'Default(1)'],\n",
        "                                    normalize = False, title = 'KNN')\n",
        "plt.savefig('knn_cm_plot.png')\n",
        "plt.show()\n",
        "\n",
        "lr_cm_plot = plot_confusion_matrix(lr_matrix,\n",
        "                                   classes = ['Non-Default(0)', 'Default(1)'],\n",
        "                                   normalize = False, title = 'Logistic Regression')\n",
        "plt.savefig('lr_cm_plot.png')\n",
        "plt.show()\n",
        "\n",
        "#3. Logistic Regression\n",
        "\n",
        "lr_cm_plot = plot_confusion_matrix(lr_matrix,\n",
        "                                   classes = ['Non-Default(0)', 'Default(1)'],\n",
        "                                   normalize = False, title = 'Logistic Regression')\n",
        "plt.savefig('lr_cm_plot.png')\n",
        "plt.show()\n",
        "\n",
        "#4. Support Vector Machine\n",
        "\n",
        "svm_cm_plot = plot_confusion_matrix(lr_matrix,\n",
        "                                    classes = ['Non-Default(0)', 'Default(1)'],\n",
        "                                    normalize = False, title = 'Logistic Regression')\n",
        "plt.savefig('lr_cm_plot.png')\n",
        "plt.show()\n",
        "\n",
        "#5. Random Forest Tree\n",
        "\n",
        "rf_cm_plot = plot_confusion_matrix(rf_matrix,\n",
        "                                   classes = ['Non-Default(0)', 'Default(1)'],\n",
        "                                   normalize = False, title = 'Random Forest Tree')\n",
        "plt.savefig('rf_cm_plot.png')\n",
        "plt.show()\n",
        "\n",
        "#6. XGBoost\n",
        "\n",
        "xgb_cm_plot = plot_confusion_matrix(xgb_matrix,\n",
        "                                    classes = ['Non-Default(0)', 'Default(1)'],\n",
        "                                    normalize = False, title = 'XGBoost')\n",
        "plt.savefig('xgb_cm_plot.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2pqnfAO3UUc"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}